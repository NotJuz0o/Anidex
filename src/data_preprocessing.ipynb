{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header_section",
   "metadata": {},
   "source": [
    "# Dataset Processing Pipeline for Image Classification\n",
    "\n",
    "## Overview\n",
    "This notebook processes raw image folders into a machine learning-ready pickle dataset.\n",
    "\n",
    "### Features:\n",
    "- Loads images from class folders\n",
    "- Resizes all images to 128×128 pixels\n",
    "- Balances classes to 1000 images each\n",
    "- Saves as pickle file for fast loading\n",
    "\n",
    "### Input Structure:\n",
    "```\n",
    "data/\n",
    "├── butterfly/\n",
    "├── cat/\n",
    "├── chicken/\n",
    "└── ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports_section",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "347c083f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_section",
   "metadata": {},
   "source": [
    "## 2. Configuration Parameters\n",
    "\n",
    "Main settings for dataset processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fa4553e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"../data\"  # Input folder with class subfolders\n",
    "IMAGE_SIZE = (128, 128)   # Target size for all images\n",
    "MAX_IMAGES_PER_CLASS = 1000  # Limit per class for balancing\n",
    "OUTPUT_PATH = \"../data/dataset.pkl\"  # Output pickle file\n",
    "SUPPORTED_EXTENSIONS = {'.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tiff'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functions_section",
   "metadata": {},
   "source": [
    "## 3. Processing Functions\n",
    "\n",
    "Core functions for image loading, filtering, and dataset creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e68bfcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_resize_image(image_path, target_size):\n",
    "    \"\"\"\n",
    "    Load image, convert to RGB, and resize to target size.\n",
    "    Returns None if image cannot be processed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "            img_resized = img.resize(target_size, Image.Resampling.LANCZOS)\n",
    "            return np.array(img_resized)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_image_files(folder_path):\n",
    "    \"\"\"\n",
    "    Get all image files from a folder based on supported extensions.\n",
    "    \"\"\"\n",
    "    image_files = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if os.path.splitext(file.lower())[1] in SUPPORTED_EXTENSIONS:\n",
    "            image_files.append(os.path.join(folder_path, file))\n",
    "    return image_files\n",
    "\n",
    "def process_dataset():\n",
    "    \"\"\"\n",
    "    Process all class folders and create balanced dataset.\n",
    "    Returns images (X), labels (y), and class names.\n",
    "    \"\"\"\n",
    "    # Find all class folders\n",
    "    class_folders = [item for item in os.listdir(DATASET_PATH) \n",
    "                    if os.path.isdir(os.path.join(DATASET_PATH, item))]\n",
    "    class_folders.sort()    \n",
    "    print(f\"\\nClasses found: {class_folders}\")\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    # Process each class\n",
    "    for class_name in class_folders:\n",
    "        print(f\"\\nProcessing: {class_name}\")\n",
    "        class_path = os.path.join(DATASET_PATH, class_name)\n",
    "        image_files = get_image_files(class_path)\n",
    "        \n",
    "        # Limit images per class for balancing\n",
    "        if len(image_files) > MAX_IMAGES_PER_CLASS:\n",
    "            image_files = random.sample(image_files, MAX_IMAGES_PER_CLASS)\n",
    "        \n",
    "        # Process each image\n",
    "        for img_path in tqdm(image_files, desc=f\"Processing {class_name}\"):\n",
    "            img_array = load_and_resize_image(img_path, IMAGE_SIZE)\n",
    "            if img_array is not None:\n",
    "                X.append(img_array)\n",
    "                y.append(class_name)\n",
    "        \n",
    "        print(f\"  {len([f for f in image_files if load_and_resize_image(f, IMAGE_SIZE) is not None])} images added !\")\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    print(f\"\\nResult:\")\n",
    "    print(f\"X shape: {X.shape}\")\n",
    "    print(f\"y shape: {y.shape}\")\n",
    "    print(f\"Classes: {len(class_folders)}\")\n",
    "    print(f\"Example y: {y[:5]}\")\n",
    "    \n",
    "    return X, y, class_folders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving_section",
   "metadata": {},
   "source": [
    "## 4. Save Dataset\n",
    "\n",
    "Save processed dataset as pickle file for fast loading during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd0e67f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(X, y, class_names, output_path):\n",
    "    \"\"\"\n",
    "    Save dataset as pickle file with images, labels, and class names.\n",
    "    \"\"\"\n",
    "    dataset = {\n",
    "        'X': X,\n",
    "        'y': y,\n",
    "        'class_names': class_names\n",
    "    }\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(dataset, f)\n",
    "    \n",
    "    file_size = os.path.getsize(output_path) / (1024**2)\n",
    "    print(f\"\\nDataset saved: {output_path} ({file_size:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "execution_section",
   "metadata": {},
   "source": [
    "## 5. Execute Pipeline\n",
    "\n",
    "Run the complete processing pipeline with reproducible random sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "348ed052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting processing...\n",
      "\n",
      "Classes found: ['butterfly', 'cat', 'chicken', 'cow', 'dog', 'elephant', 'horse', 'sheep', 'spider', 'squirrel']\n",
      "\n",
      "Processing: butterfly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing butterfly:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing butterfly: 100%|██████████| 1000/1000 [00:03<00:00, 262.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1000 images added !\n",
      "\n",
      "Processing: cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cat: 100%|██████████| 1000/1000 [00:08<00:00, 124.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1000 images added !\n",
      "\n",
      "Processing: chicken\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chicken: 100%|██████████| 1000/1000 [00:01<00:00, 581.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1000 images added !\n",
      "\n",
      "Processing: cow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cow: 100%|██████████| 1000/1000 [00:01<00:00, 614.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1000 images added !\n",
      "\n",
      "Processing: dog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dog: 100%|██████████| 1000/1000 [00:01<00:00, 566.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1000 images added !\n",
      "\n",
      "Processing: elephant\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing elephant: 100%|██████████| 1000/1000 [00:02<00:00, 381.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1000 images added !\n",
      "\n",
      "Processing: horse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing horse: 100%|██████████| 1000/1000 [00:01<00:00, 617.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1000 images added !\n",
      "\n",
      "Processing: sheep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sheep: 100%|██████████| 1000/1000 [00:02<00:00, 424.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1000 images added !\n",
      "\n",
      "Processing: spider\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing spider: 100%|██████████| 1000/1000 [00:01<00:00, 525.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1000 images added !\n",
      "\n",
      "Processing: squirrel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing squirrel: 100%|██████████| 1000/1000 [00:01<00:00, 605.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1000 images added !\n",
      "\n",
      "Result:\n",
      "X shape: (10000, 128, 128, 3)\n",
      "y shape: (10000,)\n",
      "Classes: 10\n",
      "Example y: ['butterfly' 'butterfly' 'butterfly' 'butterfly' 'butterfly']\n",
      "\n",
      "Dataset saved: ../data/dataset.pkl (469.09 MB)\n",
      "\n",
      "✅ Completed!\n",
      "📁 Dataset: ../data/dataset.pkl\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Set random seeds for reproducibility\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    print(\"🚀 Starting processing...\")\n",
    "    X, y, class_names = process_dataset()\n",
    "    save_dataset(X, y, class_names, OUTPUT_PATH)\n",
    "    print(f\"\\n✅ Completed!\")\n",
    "    print(f\"📁 Dataset: {OUTPUT_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
